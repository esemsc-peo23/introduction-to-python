{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esemsc-peo23/introduction-to-python/blob/main/Assessment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Assessment 1**\n",
        "\n",
        "For our first assessment, our goal is to solve an imputation problem: we will create a neural network architecture that learns how to recover missing portions of an image.\n",
        "\n",
        "This is an important problem in magnetic resonance imaging (MRI), where patient scans are often limited to a few areas to avoid lengthy scanning times.\n",
        "\n",
        "In particular, we are going to focus on images of human heads. We have managed to gain access to one hundred images of patient's heads but, unfortunately, these images have a significant portion of missing information. Your goal during the assessment is to design a neural network that can recover these missing portions.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "We do not have access to the labels for the images we want to recover, so we will have to be a bit creative to obtain a workable dataset on which to train our neural network.\n",
        "\n",
        "Fortunately for us, we have access to a generative model that has been trained to produce realistic-looking MRI images of patient's heads. Using this model, you will create an appropriate dataset to train your architecture. We have provided you with the basic setup code to start using this generative model in **Question 1** below.\n",
        "\n",
        "The corrupted images that we want to recover are contained in the numpy file `test_set.npy` of this repository. The file contains 100 patient images with a size of 64x64 pixels.\n",
        "\n",
        "The architecture that you design in this assessment should use the artificially-generated dataset in order to recover the missing information in the images contained in `test_set.npy`.\n",
        "\n",
        "<br>\n",
        "\n",
        "All answers to the assessment should be contained within the structure below, but you are free to add new code and text cells as required to your answers. Read the text for each question and follow the instructions carefully. Answers that do not follow this structure will not be marked. **Do NOT change the name of this file.**\n",
        "\n",
        "Please, **make sure to execute all your cells and save the result of the execution**. We will only mark cells that have been executed and will not execute any cells ourselves.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "-QqVRXQpJ8pd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 1**  (25%)\n",
        "\n",
        "Using the provided image-generation network, create a dataset of brain images that will later be used to train your chosen architecture.\n",
        "\n",
        "Given that you will likely want to use this dataset multiple times during training, we recommend that you save the generated images to an appropriate folder in your GDrive.\n",
        "\n",
        "Once you have generated your dataset, load and display ten of your generated images here.\n",
        "\n",
        "We have also provided you with some corrupted images in the file `test_set.npy` of this repository. You should also load and display ten of these corrupted images here.\n",
        "\n",
        "Below, we have provided template code, including some required downloads and installations, so that you can easily use the trained generative model. Sample generation in this model is done using the function `generate`, and is controlled by some input arguments. It is your job to figure out a sensible set of parameters that will produce images that are useful for the requirements of your task.\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "N7wmUvz7Ssz1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dDzTDiZyJ6Fp"
      },
      "outputs": [],
      "source": [
        "#importing necessary packages\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mounting google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "A0Whr9g1Vffe",
        "outputId": "9f68b463-9b44-456b-8a2b-fc02ac30e157",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting the ese_invldm package\n",
        "!unzip /content/drive/MyDrive/cw1_files.zip -d ./\n",
        "!chmod +x run.sh\n",
        "!bash ./run.sh\n",
        "sys.path.append('/content/ese-invldm')"
      ],
      "metadata": {
        "id": "W5L9telFOyhv",
        "outputId": "26ed74a0-81c9-4c66-9879-6380c0e1f1f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/cw1_files.zip\n",
            "  inflating: ./files/config_training.yml  \n",
            "  inflating: ./run.sh                \n",
            "  inflating: ./ese-invldm/setup.py   \n",
            "  inflating: ./files/default_config.yml  \n",
            "  inflating: ./ese-invldm/ese_invldm/ese_invldm.py  \n",
            "  inflating: ./ese-invldm/ese_invldm/__init__.py  \n",
            "  inflating: ./files/autoencoder/autoencoder_ckpt_latest.pth  \n",
            "  inflating: ./files/diffusion/diffusion_ckpt_latest.pth  \n",
            "Cloning the repository from https://github.com/dpelacani/InverseLDM.git...\n",
            "Cloning into 'InverseLDM'...\n",
            "remote: Enumerating objects: 1331, done.\u001b[K\n",
            "remote: Counting objects: 100% (261/261), done.\u001b[K\n",
            "remote: Compressing objects: 100% (161/161), done.\u001b[K\n",
            "remote: Total 1331 (delta 167), reused 182 (delta 100), pack-reused 1070 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1331/1331), 467.16 KiB | 13.35 MiB/s, done.\n",
            "Resolving deltas: 100% (875/875), done.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.9/175.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.4/52.4 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a directory to save my generated images\n",
        "save_dir = '/content/drive/MyDrive/generated_images'\n",
        "os.makedirs(save_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "gVMv4foCPWki"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generating the uncorrupted images and saving them to the drive\n",
        "from ese_invldm import generate\n",
        "\n",
        "\"\"\"\n",
        "Generates samples using a diffusion-based generative model.\n",
        "\n",
        "This function leverages a pre-configured diffusion model to produce synthetic samples.\n",
        "The sampling process supports adjustable parameters for total samples, inference steps, and batch size.\n",
        "A scheduler and temperature can also be configured to control the sampling behaviour.\n",
        "\n",
        "Parameters:\n",
        "    num_samples (int):\n",
        "        Total number of samples to generate.\n",
        "    num_inference_steps (int):\n",
        "        Number of diffusion inference steps.\n",
        "        The minimum number of steps is 1, but we recommend exploring the range from 10 to 50.\n",
        "        Please note that more steps will increase quality but also the computational cost. Be careful not\n",
        "        to burn through your credits by using a very large number of steps!\n",
        "    batch_size (int):\n",
        "        Number of samples to process in each batch during sampling.\n",
        "    scheduler (str, optional):\n",
        "        Sampling scheduler to use (e.g., \"ddim\", \"ddpm\"). You can quickly test which one provides the most appropriate results\n",
        "        for this task.\n",
        "    temperature (float, optional):\n",
        "        Sampling temperature to control randomness, given as a number between 0 and 1. Higher values produce more diverse outputs.\n",
        "    seed (int, optional):\n",
        "        Random seed for reproducibility. Defaults to 42.\n",
        "\n",
        "Returns:\n",
        "    list:\n",
        "        A list containing the batches of generated samples, where each sample\n",
        "        corresponds to a single data instance produced by the diffusion model.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "samples = generate(num_samples=5, num_inference_steps=1, batch_size=5, scheduler='ddim', temperature= 0.5)\n",
        "\n",
        "# Convert samples to a list of numpy arrays for plotting\n",
        "image_nps = [img.cpu().numpy().squeeze(0) for img in samples]\n",
        "\n",
        "# Calculate grid dimensions\n",
        "num_images = 10\n",
        "rows = 1\n",
        "cols = 10\n",
        "\n",
        "# Create figure and subplots\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(12, 12))\n",
        "\n",
        "# Flatten axes for easier iteration\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Plot images\n",
        "for i, image in enumerate(image_nps):\n",
        "    ax = axes[i]\n",
        "    ax.imshow(image.squeeze(), cmap='gray')  # Display image\n",
        "    ax.axis('off')  # Hide axes\n",
        "\n",
        "# Hide any unused subplots\n",
        "for i in range(num_images, rows * cols):\n",
        "    axes[i].axis('off')\n",
        "\n",
        "# Set title\n",
        "fig.suptitle(\"Generated Images\", fontsize=16)\n",
        "\n",
        "# Adjust layout and show plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#     # Save the image as a PNG file\n",
        "#     file_path = os.path.join(save_dir, f'image_{i}.png')\n",
        "#     plt.imsave(file_path, image_np, cmap='gray')\n",
        "\n",
        "# print(f\"Images saved to: {save_dir}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "6KbXjEepVhTO",
        "outputId": "7982be0a-81c1-4eba-e1cc-3b331e1f593e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/InverseLDM/invldm/runners/base_runner.py:137: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_states = torch.load(path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  6.36it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "cannot select an axis to squeeze out which has size not equal to one",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-23c3582bc86e>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Convert samples to a list of numpy arrays for plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mimage_nps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Calculate grid dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-23c3582bc86e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Convert samples to a list of numpy arrays for plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mimage_nps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Calculate grid dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot select an axis to squeeze out which has size not equal to one"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(samples)"
      ],
      "metadata": {
        "id": "cyAz_WeTQ1Ns",
        "outputId": "4bf320c3-cc1d-4282-9d2b-660756fb4b6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[[[0.0300, 0.0660, 0.0778,  ..., 0.0900, 0.0802, 0.0595],\n",
            "          [0.0388, 0.1171, 0.1974,  ..., 0.2520, 0.1575, 0.0935],\n",
            "          [0.0883, 0.1366, 0.2365,  ..., 0.3652, 0.1731, 0.0944],\n",
            "          ...,\n",
            "          [0.0645, 0.0770, 0.1447,  ..., 0.1499, 0.1063, 0.0917],\n",
            "          [0.0634, 0.0669, 0.1447,  ..., 0.1146, 0.0612, 0.0591],\n",
            "          [0.0468, 0.0537, 0.0839,  ..., 0.0665, 0.0463, 0.0383]]],\n",
            "\n",
            "\n",
            "        [[[0.0317, 0.0337, 0.0315,  ..., 0.0571, 0.0409, 0.0325],\n",
            "          [0.0323, 0.0378, 0.0409,  ..., 0.0958, 0.0765, 0.0508],\n",
            "          [0.0323, 0.0488, 0.0934,  ..., 0.1991, 0.1831, 0.1039],\n",
            "          ...,\n",
            "          [0.0789, 0.0772, 0.0528,  ..., 0.2642, 0.1648, 0.1057],\n",
            "          [0.0629, 0.0526, 0.0456,  ..., 0.1616, 0.1423, 0.1110],\n",
            "          [0.0405, 0.0385, 0.0367,  ..., 0.1289, 0.1338, 0.0964]]],\n",
            "\n",
            "\n",
            "        [[[0.0280, 0.0318, 0.0326,  ..., 0.0594, 0.0447, 0.0385],\n",
            "          [0.0377, 0.0499, 0.0440,  ..., 0.0715, 0.0603, 0.0453],\n",
            "          [0.0659, 0.0869, 0.0677,  ..., 0.0813, 0.0673, 0.0574],\n",
            "          ...,\n",
            "          [0.0307, 0.0405, 0.0429,  ..., 0.0552, 0.0805, 0.0820],\n",
            "          [0.0306, 0.0363, 0.0401,  ..., 0.0536, 0.0659, 0.0602],\n",
            "          [0.0285, 0.0306, 0.0354,  ..., 0.0720, 0.0852, 0.0540]]],\n",
            "\n",
            "\n",
            "        [[[0.0287, 0.0286, 0.0285,  ..., 0.0586, 0.0402, 0.0343],\n",
            "          [0.0311, 0.0366, 0.0383,  ..., 0.2094, 0.0718, 0.0447],\n",
            "          [0.0339, 0.0376, 0.0435,  ..., 0.5391, 0.2004, 0.0631],\n",
            "          ...,\n",
            "          [0.2499, 0.3167, 0.5015,  ..., 0.0757, 0.0588, 0.0404],\n",
            "          [0.0988, 0.2180, 0.4971,  ..., 0.0524, 0.0426, 0.0308],\n",
            "          [0.1036, 0.1770, 0.4290,  ..., 0.0464, 0.0381, 0.0281]]],\n",
            "\n",
            "\n",
            "        [[[0.0371, 0.0670, 0.0723,  ..., 0.0320, 0.0466, 0.0498],\n",
            "          [0.0534, 0.1758, 0.2644,  ..., 0.0512, 0.0690, 0.0853],\n",
            "          [0.0845, 0.2394, 0.4426,  ..., 0.0703, 0.0789, 0.0961],\n",
            "          ...,\n",
            "          [0.0468, 0.0592, 0.0771,  ..., 0.2065, 0.1882, 0.1045],\n",
            "          [0.0373, 0.0490, 0.0528,  ..., 0.1588, 0.1185, 0.0959],\n",
            "          [0.0479, 0.0724, 0.0632,  ..., 0.1242, 0.1436, 0.1224]]]],\n",
            "       device='cuda:0', dtype=torch.float16)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    plt.imshow(image_np, cmap='gray')  # Displaying the image using the grayscale colormap\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "jz4URaYQSFx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the first 10 images\n",
        "for i in range(5):\n",
        "    # Squeezing the image to remove the extra dimension\n",
        "    image = samples_1[i]\n",
        "    plt.imshow(image, cmap='gray')  # Displaying the image using the ygrayscale colormap\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "H3VEqKGJvbQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the test dataset\n",
        "\n",
        "test_set = np.load('/content/gdrive/MyDrive/test_set.npy')\n",
        "\n",
        "# Iterate through the first 10 images\n",
        "for i in range(10):\n",
        "    # Squeezing the image to remove the extra dimension\n",
        "    image = np.squeeze(test_set[i])\n",
        "    plt.imshow(image, cmap='gray')  # Displaying the image using the ygrayscale colormap\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "EGDmZmGuhGBD",
        "outputId": "99571e89-7bc7-4c5c-fefb-8503e11f571a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/gdrive/MyDrive/test_set.npy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-4451cbbd1073>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loading the test dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/MyDrive/test_set.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Iterate through the first 10 images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/MyDrive/test_set.npy'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Question 2**  (25%)\n",
        "\n",
        "Using the data generated in **Question 1**, create a PyTorch `TensorDataset` and a `DataLoader` for the training set.\n",
        "\n",
        "Using the provided corrupted images inside `test_set.npy`, create another `TensorDataset` and a `DataLoader` for the test set.\n",
        "\n",
        "The training dataset should provide batches of brain images generated in **Question 1** and should corrupt these images appropriately so that they resemble images in the test set. The dataset should also pair each image with its corresponding un-corrupted image as a label.\n",
        "\n",
        "The test dataset should provide the corrupted images provided, for which no labels are available.\n",
        "\n",
        "Display here ten images of your training dataset and ten images of your test dataset, and their corresponding labels when available.\n",
        "\n",
        "<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "mBVr6D2oSInX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing packages required to corrupt the images\n",
        "!pip install pycm livelossplot torchinfo -q\n",
        "%pylab inline\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "from livelossplot import PlotLosses\n",
        "\n",
        "import matplotlib.animation as animation\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torchinfo import summary\n",
        "import torchvision\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "hFfCml21nNkr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12ae1c42-102f-4904-b496-dcd7863579a7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.6/608.6 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hPopulating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NuETd-X2SKPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Question 3** (50%)\n",
        "\n",
        "Using the dataset created in **Question 2**, design and train an architecture to recover the missing image lines of the provided test dataset.\n",
        "\n",
        "Once you have trained your architecture, display here ten images of the test set with the recovered lines filled in.\n",
        "\n",
        "Additionally, save the test data with the missing values filled in into a numpy file called `test_set_nogaps.npy`. These images should be **in the same order** as those in the `test_set.npy` file and should have the same pixel size of 64x64. **Any images not contained in the `test_set_nogaps.npy` file or incorrectly ordered will not be marked.**\n",
        "\n",
        "You have freedom to choose an architecture that you consider appropriate to solve this problem. However, you will need to train your chosen architecture as part of the assessment: **pre-trained networks are not allowed**.\n",
        "\n",
        "You will be assessed by the quality of your predictions of the missing data values and additional marks will be given for originality in your network design choices. You should include, as part of your answer, a paragraph explaining the architecture you have chosen and any additional design choices and hyperparameters that have been important to build your solution.\n",
        "\n",
        "This is an open-book assessment and you are encouraged to use resources online, including  tools like chatGPT. However, make sure to always mention the sources for your code and ideas, including websites, papers, and tools like chatGPT.\n",
        "\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "1-AoRoLUSKoH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KFNwYx_hSMG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "5cdZQIkWu0_r"
      }
    }
  ]
}